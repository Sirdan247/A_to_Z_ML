#!/usr/bin/env python
# coding: utf-8

# In[1]:


# Data Preprocessing

# import the libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# import the dataset
dataset = pd.read_csv('Data.csv')
dataset.head()

# X = dataset.iloc[:,:-1]
# y = dataset.iloc[:, 3]



# Encoding categorical data
# Use only LabelEncoder for dependent vector (y) and for any ranked independent matrix (X)
# from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# labelencoder_X = LabelEncoder()
# X['Country'] = labelencoder_X.fit_transform(X['Country'])
# onehotencoder = OneHotEncoder(categorical_features = [0])
# X = pd.DataFrame(onehotencoder.fit_transform(X).toarray())

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer

ct = ColumnTransformer(
    [('one_hot_encoder', OneHotEncoder(), [0])],    # The column numbers to be transformed (here is [0] but can be [0, 1, 3])
    remainder='passthrough'                         # Leave the rest of the columns untouched
)

X = pd.DataFrame(np.array(ct.fit_transform(X), dtype=np.int))

le_y = LabelEncoder()
y = pd.DataFrame(le_y.fit_transform(y), columns =['Purchased'])



# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split # Used to be sklearn.cross_validation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)



# Feature scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X = sc_X.fit_transform(X)
y = sc_y.fit_transform(y)

